xlattice_go/node/TODO

2013-11-16
    * IDMap should synchronize access internally                        * DONE
    * should return a count (synchronized)                              * DONE
    * add testing the count to id_map_test.go

2013-11-16 (from 2013-08-23, 2013-09-30, edited)
    * bni_map: consider replacing the current approach, perhaps with	* DONE
        one where we have an array of 2^N pointers at each level -		* DONE
        should be simpler to construct and much faster to search		* DONE
        - edit to use IDMap (cost is redundant reference to ID)		    * DONE
        - later revise to become a simplified IDMap that just uses		* DONE
            the ID in BaseNodeI, dropping the superfluous Key *[]byte

2013-09-19
    * first NextNBLine check in Peer.collectConnectors should be
        peek !

2013-09-17
    * Peer.Equal() needs to be fleshed out
    * BaseNode.Equal() also needs to be fleshed out

2013-09-16
    * BaseNode.Equal() was wrong and needs to be fixed further

2013-08-29
    * The Peer 'up' field should be a field of bit flags
        (declare as int but only use 32 bits)

2013-08-25
    * complete the stubbed sections in
        test_node.TestAutoCreateOverlays

2013-08-19
    * arguably need stronger version of Peer.Equal(); it only
        confirms that NodeIDs are the same

2013-08-15
    * document this: running localHost_test.go with Q=64 now just works
        - on Ubuntu edit /etc/security/limits.conf adding
            * soft nofile 16384
            * hard nofile 65536
        - Panicked on test; then made the above change, rebooted, and
            tried again.  Succeeded as normal user.
    * Added code to set MAXPROC to 8 at the beginning of each unit
        test.  Definitely improved cpu usage as reported by top,
        usually by 2 or 3% but peak usage went from 100% to 127%
        on a 2-cpu system (not yet tested on 4-cpu).  However,
        local_host_test line 96 now blows up with
        "connection reset by peer" within nodeAsClient(), invoked
        by TestLocalHostTcpCluster().
        - if I reset MAXPROC to 1 or 2, the test succeeds :-)
        - at MAXPROC of 3 to 5 it sometimes blows up; higher
            values seem to always blow up
        - if I do two test runs at the same time, both blow up
            * this suggests that Go multi-threading is not safe
        - if I run six tests simultaneouwly by detaching with
            ampersand, at least one blows up with connection
            errors.  CPU usage is good, totalling about 200%
            with 2 cores.
        - I ran eight tests simultaneously: 1 failed with connection
            errors, 2 failed because a listening address was already
            in use when deserializing
        _ Reran 8 detached, 1 failed with "connection reset by peer"
        _ Reran 8 detached, 1 failed with 'address already in use'
            in serialization/deserialization test

2013-08-12
    * In order to force an orderly shutdown of a Node, it must be
        doing something.  In other words, we need a set of commands
        which take a node through a series of states: probably
        Load(), Run(), Close(), Shutdown(), and Save(), where the
        third closes all connections and the fourth releases other
        resources.  This approach precludes or at least constrains
        controlling a node through one of those connections.

    * While the test is running, CPU usage goes up to 100% -
        which would seem to mean it's only using 1 CPU out of the
        8 available.  Not good.

    * while the test was running lsof | wc -l returned 4735
        - after the test it returned 4710 or so for quite some time

    * FWIW free memory was about 26 GB before, during, and after the test
        (on a 8-core system with 32 GB of RAM)

    * edited /etc/security/limits.conf, no benefit, may require reboot

2013-08-10
    * implement node.Save() and node.Load()
        - Save() is not just a SnapShot(): it forces an orderly
            shutdown of the node

    * if a node is started with only the name, id, and lfs
        specified, it should load its configuration from say
        LFS/.xlattice/config

    * extract localHost cluster configuration from localHost_test

    * extend localHostTest so that after configurations have
        been saved, all of the nodes in the cluster are shut
        down and then all are restarted from their saved
        configurations.  The save-and-reload should precede
        the communications checks.

2013-07-30
    * p_test.go should be done over the wire, so to speak; that
        is, between two 127.0.0.1 endpoints
    * the protocol must be documented
    * integrating gateways into node requires ability to create
        composite connections
        - Gateway extends Node
    * need UDP transport

2013-07-29
    * Node.Close must close all open connections
        - take care, servers may have MANY such connections
            * could put each Close() in separate goroutine
